<!doctype html>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- ---- paulirish.com/2008/conditional-stylesheets-vs-css-hacks-answer-neither/ -->
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!-- Consider adding an manifest.appcache: h5bp.com/d/Offline -->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
  <meta charset="utf-8">

  <!-- Use the .htaccess and remove these lines to avoid edge case issues.
       More info: h5bp.com/b/378 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Visualizing and Understanding Recurrent Networks |  Volkan Cirik</title>
  <meta name="author" content="Volkan Cirik">
  

  
  <!-- Mobile viewport optimized: j.mp/bplateviewport -->
  <meta name="viewport" content="width=device-width,initial-scale=1">

  <!-- Place favicon.ico and apple-touch-icon.png in the root directory: mathiasbynens.be/notes/touch-icons -->

  <!-- CSS: implied media=all -->
  <!-- CSS concatenated and minified via ant build script-->
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Inconsolata' rel='stylesheet' type='text/css'>
  <!-- end CSS-->

  <!-- More ideas for your <head> here: h5bp.com/d/head-Tips -->

  <!-- All JavaScript at the bottom, except for Modernizr / Respond.
       Modernizr enables HTML5 elements & feature detects; Respond is a polyfill for min/max-width CSS3 Media Queries
       For optimal performance, use a custom Modernizr build: www.modernizr.com/download/ -->
  <script src="/javascripts/libs/modernizr-2.0.6.min.js"></script>
</head>

<body id="post">

  <div id="container">
    <nav role="navigation" ><ul class="main-navigation">
  <li style='float:left;margin-right:5px'><a class='nav' href="/">Home</a></li>
</ul><br><br>
</nav>
    <article>


  <h1 class="article-title">Visualizing and Understanding Recurrent Networks</h1>
  <div class="article-content"><p>Third reading of the semester is from Karpathy et. al. <a href="http://arxiv.org/pdf/1311.2901v3.pdf">Visualizing and Understanding Recurrent Networks
</a>.
I can’t write <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">a blog post</a> better than the first author. But, the paper is more than learning/having fun with RNNs, it provides valuable insights about how RNNs work.
<a href="http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139">A follow-up post</a> from Goldberg is complementing the message conveyed in the paper.
If you would like to cover the basics about sequential modelling with RNN these three readings are good starting points.</p>

<hr />

<p><img src="https://dl.dropboxusercontent.com/u/13917601/pap3_f1.png" alt="fig1" /></p>

<p><strong>Deeper Models with Gating Mechanism are Better:</strong>  They compare vanilla RNN, GRU, and LSTM on Linux Kernel (LK) and Tolstoy’s War and Peace (WP) using character-level language modelling as a benchmark. In this <strong>fair comparison</strong>, gated models (I will refer LSTMs and GRUs as Gated RNNs from now on) are better than vanilla RNN. In addition, the depth of at least two is suggested. See above table.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/pap3_f2.png" alt="fig2" /></p>

<p><strong>Some Cells have Specific Roles:</strong> As seen above,  for $tanh(c)$ where -1 is red and +1 is blue, some cells are specializing in capturing some phenomenon such as counting lines, detecting quotes etc.  <strong>Question:</strong> what are other cells doing? <strong>Speculation:</strong> My bet is on helping approximation the function we would like to learn. They are not serving as a memory they are helping the computation.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/pap3_f3.png" alt="fig3" /></p>

<p><strong>In a Deep Gated RNN Lower Layers Don’t Employ Gating Mechanism:</strong> Let’s define left/right saturation for a gate. For a gate activation frequency, $0 \leq g_a \leq 1$, we say the gate is left saturated if $g_a &lt; 0.1$ and right saturated $g_a &gt; 0.9$. In English, if it’s right saturated corresponding gate information flows in (gate is almost always closed). If it’s left saturated gate often does not let information flow in (gate is open). Above figure show that in a deep net, lower layers do not employ gating mechanism. <strong>Question</strong> what if we train well a single layer gated RNN, do we see the same phenomenon? <strong>Speculation</strong> My bet is that we will not. The complexity of the model will change the characteristics of each layer. Thus, single layer gated RNN will have some gating cells.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/pap3_f5.png" alt="fig4" /></p>

<p><strong>Gated RNNs Learn To Generalize for Longer Sequences:</strong> One of the best news from this paper, it is nice to see a verification of common knowledge. Neural Network-based n-gram model overfits and fails to beat traditional n-gram model. However, for sequences greater than n, n-gram fails to generalize unlike gated RNN. RNNs history help it utilize information beyond 20 characters (See also Table 2 and Figure I in paper).</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/pap3_f4.png" alt="fig4" /></p>

<p><strong>Gated RNNs Learn Special Functions Hinted in Data:</strong> When characters have special functions in data such as <em>”@,{,\r”</em>, gated RNN significantly better than n-gram model at predicting them. This hints that they probably learn to model their functionality. <strong>Question:</strong> What if each symbol has its own set of specific function? <strong>Speculation:</strong> We’ll see an increase in number of saturated/specializes cells in the network. A synthetic task may help us prove/disprove this speculation.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/pap3_f6.png" alt="fig6" /></p>

<p><strong>Gated RNNs Learn to Deal With Short-range Dependencies First</strong>: They use a brilliant way to show that Gated RNNs behave like a lower order n-gram NN first and generalize for higher orders later. Above graph shows the comparison of the similarity between a deep LSTM and the n-NN baselines over the early epochs of training. On the left the symmetric KL-divergence between models (how similar the loss distributions/predictions are) is shown. On the right, the difference in the mean loss is shown (positive loss means RNN outperforms).</p>

<p><strong>Gated RNNs are not super efficient at utilizing the history:</strong> Section 4.4 of the paper shows a nice Error Analysis of the model – which is not common in DL community. They show how to breakdown errors types and conduct oracle experiments. My takeaway from that part is an n-gram help us eliminate 18% of errors. This shows that gated RNNs do not fully utilize the previous context. They fail to compress the history of seen characters even for the previous 9 characters. <strong>Question:</strong> Could there be a better memory-based model in addressing this? <strong>Speculation:</strong> Of course!</p>

</div>
  
    <section>
      <h1>Comments</h1>
      <div id="disqus_thread"><div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'wolet-github';
  var disqus_identifier = 'http://wolet.github.com/blog/visualizing-and-understanding-recurrent-networks/';
  var disqus_url = 'http://wolet.github.com/blog/visualizing-and-understanding-recurrent-networks/';
  //var disqus_developer = 1;
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
    </section>
  

</article>


      <p class="meta">
      
        <a class="basic-alignment left nav" href="/blog/visualizing-and-understanding-convolutional-neural-networks/" title="Previous Post: Visualizing and Understanding Convolutional Neural Networks">&laquo; Visualizing and Understanding Convolutional Neural Networks</a>
      
      
        <a class="basic-alignment right nav" href="/blog/book-review-now-the-physics-of-time/" title="Next Post: Book Review: Now: The Physics of Time">Book Review: Now: The Physics of Time &raquo;</a>
      
    </p>
  </div>
    <script>
      var name = document.getElementById('name-helper').innerHTML;

      var canvas = document.getElementById('myCanvas');

      //The name (up to 28 characters long, including white space)
      var length = name.length;
      if (length>28){//Exceed character length limit, remove canvas
          canvas.remove();
      }
      else{
            

        var context = canvas.getContext('2d');

        //calculate width and length
        var width=700/1.25/length;
        var start;
        if (width>40){
          width=40;
        }

        var height= width/2;

        //the start postion of the link list
        var start = (700-width*length*1.25)/2;
        var horizon = 5;

        //DO NOT CHANGE THOSE
        var mid_horizon = horizon+height/2;
        var name_counter =0;

        var x = height/5;
        var y = height/5;
        var interval=width/4;
        
        var radius = 0.75*width;
        
        var text_pos=width*1.1;


        canvas.height=mid_horizon+width/2+width/2.1+5;
        
        for (var i=1;i<name.length+1;i++){

          context.beginPath();
          context.strokeStyle='#F9BF3B';
          
          //the rectangle
          context.rect(start, horizon, width, height);

          //the middle line
          context.moveTo(start+width/2,horizon);
          context.lineTo(start+width/2,horizon+height);

          //if this is the last element
          if (i==name.length){
            context.moveTo(start+width/2,horizon+height);
            context.lineTo(start+width,horizon);

          }
          else{
            //the arrow
            context.moveTo(start+0.75*width,mid_horizon);
            context.lineTo(start+width+interval,mid_horizon);

            context.moveTo(start+width+interval,mid_horizon);
            context.lineTo(start+width+interval-x,mid_horizon+y);
          
            context.moveTo(start+width+interval,mid_horizon);
            context.lineTo(start+width+interval-x,mid_horizon-y); 
          }
          
       
        
          var startAngle = 1.2 * Math.PI;
          var endAngle = 1 * Math.PI;
          var counterClockwise = true;
      
          context.stroke();
            
          context.beginPath();
          context.arc(start+width/4+radius*0.81, mid_horizon+width/2, radius, startAngle, endAngle, counterClockwise);

          context.moveTo(start+width/4+radius*0.81-radius,mid_horizon+width/2);
          context.lineTo(start+width/4+radius*0.81-radius-x,mid_horizon-y+width/2);

          context.moveTo(start+width/4+radius*0.81-radius,mid_horizon+width/2);
          context.lineTo(start+width/4+radius*0.81-radius+x,mid_horizon-y+width/2); 

          context.font = ' 15pt Calibri';
          context.fillStyle = '#859900';
          context.fillText(name.charAt(name_counter), start+width/4+radius*0.81-radius-width/14, mid_horizon+width/2+width/2.1);

          context.lineWidth=1;
          context.strokeStyle='#F9BF3B';
          context.stroke();
          start+=width/2*2+interval;      
          name_counter+=1;
        }
      }

    </script>

  <!-- JavaScript at the bottom for fast page loading -->

  <!-- Grab Google CDN's jQuery, with a protocol relative URL; fall back to local if offline -->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.6.2/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="js/libs/jquery-1.6.2.min.js"><\/script>')</script>


  <!-- scripts concatenated and minified via ant build script-->
  <script defer src="/javascripts/app.js"></script>
  <!-- end scripts-->

	

  <!-- Prompt IE 6 users to install Chrome Frame. Remove this if you want to support IE 6.
       chromium.org/developers/how-tos/chrome-frame-getting-started -->
  <!--[if lt IE 7 ]>
    <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
    <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
  <![endif]-->
 

  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39225214-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>



</body>
</html>

