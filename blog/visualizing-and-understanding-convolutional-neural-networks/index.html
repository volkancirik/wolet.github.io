<!doctype html>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- ---- paulirish.com/2008/conditional-stylesheets-vs-css-hacks-answer-neither/ -->
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!-- Consider adding an manifest.appcache: h5bp.com/d/Offline -->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
  <meta charset="utf-8">

  <!-- Use the .htaccess and remove these lines to avoid edge case issues.
       More info: h5bp.com/b/378 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Visualizing and Understanding Convolutional Neural Networks |  Volkan Cirik</title>
  <meta name="author" content="Volkan Cirik">
  

  
  <!-- Mobile viewport optimized: j.mp/bplateviewport -->
  <meta name="viewport" content="width=device-width,initial-scale=1">

  <!-- Place favicon.ico and apple-touch-icon.png in the root directory: mathiasbynens.be/notes/touch-icons -->

  <!-- CSS: implied media=all -->
  <!-- CSS concatenated and minified via ant build script-->
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Inconsolata' rel='stylesheet' type='text/css'>
  <!-- end CSS-->

  <!-- More ideas for your <head> here: h5bp.com/d/head-Tips -->

  <!-- All JavaScript at the bottom, except for Modernizr / Respond.
       Modernizr enables HTML5 elements & feature detects; Respond is a polyfill for min/max-width CSS3 Media Queries
       For optimal performance, use a custom Modernizr build: www.modernizr.com/download/ -->
  <script src="/javascripts/libs/modernizr-2.0.6.min.js"></script>
</head>

<body id="post">

  <div id="container">
    <nav role="navigation" ><ul class="main-navigation">
  <li style='float:left;margin-right:5px'><a class='nav' href="/">Home</a></li>
</ul><br><br>
</nav>
    <article>


  <h1 class="article-title">Visualizing and Understanding Convolutional Neural Networks</h1>
  <div class="article-content"><p>Second reading of the semester is from Ziler &amp; Fergus <a href="http://arxiv.org/pdf/1311.2901v3.pdf">“Visualizing and Understanding Convolutional Networks”</a>. If you don’t feel comfortable with Convolutional Networks I suggest to read following readings before this paper :  <a href="http://neuralnetworksanddeeplearning.com/chap6.html">Nielsen’s Chapter on Conv Nets</a>, <a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/">Colah’s Explanation of Convolution with Visualizations</a>, and <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">Britz’s post on CNNs for NLP</a>.</p>

<p>The core idea of the paper is to use visualization of deeper layers in the ConvNet (i) to understand what’s going on under the hood and (ii) to extend them for better performance. Unlike many papers in the field, it provides lots of insights useful for further research.</p>

<hr />
<p>It is common to visualize the feature maps of the first layer in a ConvNet, but it is not trivial for further layers because of pooling. They address this using <strong>deconvnet</strong>. The main idea is to go backward (Feed-Backward??) in ConvNet.  Moving from unpooled maps to previous filter layer is straightforward : just use the transposed filter. The tricky part is the unpooling layer. How would you go back from pooling layer? You keep track of the positions of the max value for each pooling region (they name them <em>switch</em>).  Trivially, unpooling would generate blurrier images if it were mean. See below image from the paper for unpooling.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/unpooling.png" alt="unpooling" /></p>

<p>So, how would we use deconvnet for visualization? First we train a model. For a feature map learned in some layer, we pull the deconv trick and reconstruct an image that will show for which image that feature map is activated most. Let’s see what’s going on in a ConvNet when it is trained on ImageNet data.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/vis_feature_map.png" alt="feature maps" /></p>

<p>Above, for each layer some random feature maps, their top 9 activations and reconstructed images are visualized. I don’t see anything surprising in the first layer. There is some abstraction in Layer 2. But layer 3 is surprising. On the upper left, we see that feature map is specialized in grid-like shapes for texture because <em>these are the top activations of the same feature map</em>. In the context of deep learning it is common knowledge that the model uses abstraction in higher layers, and here we observe this phenomenon. My another cherry-pick would be the one in row 2, column 4. That feature map is sensitive to things with a series of things with the same height and separated by similar distances such as text or windows in a building.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/layer4-5.png" alt="4-&gt;5" /></p>

<p>However, if we take a look at layer 4 &amp; 5, the use of abstraction is not as evident as going from 2-3. I will argue that there could be a limit of abstraction in a ConvNet. It may be impossible to abstract away the spatial relationships between objects in a large image or address visual analogies with this classical model. That’s enough for speculation, let’s move on with another visualization.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/evolution.png" alt="evolution of feature maps" /></p>

<p>Above we see how 6 random feature maps for each layer are changing during training. Again, it is nice to see that, feature maps in lower layer converging faster and more abstract layers reach their final form later in training. This is a weak evidence that abstraction and feature re-using is happening in ConvNets. It would be great to show that feature maps in upper layers share some feature maps of below layer using the same deconvolution idea.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/invariance.png" alt="invariance of feature maps" /></p>

<p>Above is another useful visualization. Again, it is common knowledge that ConvNets learn invariant features. Here, they translate, scale, and rotate randomly picked 5 images (column 1). They show the distance between transformed and original feature vectors in different layers (column 2,3) and the probability of the true table for each transformation (column 4). There are too many interesting things going on here. For instance, in c4 we see random picks for some object categories but not for the lawn mower. I think it is because the model sees the lawn mowers and their handles with a certain angle during training. When it is rotated it is having difficulty to detect that it is a lawn mower. Another example is on b4. When the scale is increased too much, we are loosing the focus of tv + its context and it is hard for the model to classify it as an entertainment center. I invite you to spend some time on this graphs and images.</p>

<p><img src="https://dl.dropboxusercontent.com/u/13917601/occlusion.png" alt="occlusion" /></p>

<p>Another question is whether a ConvNet is able to locate objects or their features in an image. As show in above, they cover up different parts of the image (a), visualize how much the strongest feature maps are activated (b), what are these feature maps (c) and how would these occlusions change models prediction power (d,e). Nothing surprising but it is nice to observe that the model is able to (i) locate the most discriminative location of the image (ii) use the most relevant feature maps – feature maps in (c) are exactly what I use in my mind to predict the breed of this dog (iii) and it heavily depends on using this discriminative location.</p>

<p>Another interesting analysis is to see whether the model implicitly computes a correspondence between object parts in <em>different</em> images. For instance, eyes and nose of dogs are always observed in similar distance and rotation. Their relative distances do not drastically change (we don’t see any dog with one eye is terribly far away from the other). Interestingly, they pick only 5 dog images (not 5 types of dogs with many images) to see whether the deltas in feature calculations are moving in the same direction (clever to use Hamming distance) when left eye, right eye, and the nose is covered. They concluded that the model employs some sort of correspondence of these parts. As I speculated earlier, we may need a new layer to model these forms of spatial correspondence which can be learned from training data without extra annotation.</p>

<p>Finally, they use visualization of feature maps in different layers to see how efficient a model for learning useful feature maps. It turns out, Alexnet is wasting some of the parameters for learning non-discriminative feature maps. Visualization helps us choose hyperparameters of the model like the size of strides and feature maps to make it more accurate and more efficient in terms of parameters.</p>

<p>Using the insight they gain from visualization, they evaluate supervised pre-trained models in Caltech and Pascal benchmark. To be more explicit, they train their model on ImageNet, discard the softmax layer, and train the model with brand new softmax layer for the new dataset. It is like merging a bullet with a used shell for a new shot (terrible analogy). It turns out that the feature maps learned in ImageNet are discriminative enough to achieve great results on these benchmarks.</p>

<p>They also investigate the use of a different number of hidden units in a fully connected layer, varying the depth of convnet etc. Although I agree the claims are true (the higher number of deep convolutional layers is crucial but not for a fully connected layer), they never mention the number of parameters of each compared model, doing gridsearch to learn width of layers, or any form of cross-validation; the reported numbers are not conclusive. (Interestingly <em>very very few</em> people fairly compare different models, I guess nobody cares about model complexity.)</p>

<p>tl;dr Great paper with lots of insights on how ConvNets work. A paper with a similar quest in the context of sequence modeling is<a href="http://arxiv.org/pdf/1506.02078v2.pdf">“Visualizing and Understanding Recurrent Networks”</a>.</p>

</div>
  
    <section>
      <h1>Comments</h1>
      <div id="disqus_thread"><div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'wolet-github';
  var disqus_identifier = 'http://wolet.github.com/blog/visualizing-and-understanding-convolutional-neural-networks/';
  var disqus_url = 'http://wolet.github.com/blog/visualizing-and-understanding-convolutional-neural-networks/';
  //var disqus_developer = 1;
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
    </section>
  

</article>


      <p class="meta">
      
        <a class="basic-alignment left nav" href="/blog/tao-of-research-0/" title="Previous Post: tao of research 0">&laquo; tao of research 0</a>
      
      
    </p>
  </div>
    <script>
      var name = document.getElementById('name-helper').innerHTML;

      var canvas = document.getElementById('myCanvas');

      //The name (up to 28 characters long, including white space)
      var length = name.length;
      if (length>28){//Exceed character length limit, remove canvas
          canvas.remove();
      }
      else{
            

        var context = canvas.getContext('2d');

        //calculate width and length
        var width=700/1.25/length;
        var start;
        if (width>40){
          width=40;
        }

        var height= width/2;

        //the start postion of the link list
        var start = (700-width*length*1.25)/2;
        var horizon = 5;

        //DO NOT CHANGE THOSE
        var mid_horizon = horizon+height/2;
        var name_counter =0;

        var x = height/5;
        var y = height/5;
        var interval=width/4;
        
        var radius = 0.75*width;
        
        var text_pos=width*1.1;


        canvas.height=mid_horizon+width/2+width/2.1+5;
        
        for (var i=1;i<name.length+1;i++){

          context.beginPath();
          context.strokeStyle='#F9BF3B';
          
          //the rectangle
          context.rect(start, horizon, width, height);

          //the middle line
          context.moveTo(start+width/2,horizon);
          context.lineTo(start+width/2,horizon+height);

          //if this is the last element
          if (i==name.length){
            context.moveTo(start+width/2,horizon+height);
            context.lineTo(start+width,horizon);

          }
          else{
            //the arrow
            context.moveTo(start+0.75*width,mid_horizon);
            context.lineTo(start+width+interval,mid_horizon);

            context.moveTo(start+width+interval,mid_horizon);
            context.lineTo(start+width+interval-x,mid_horizon+y);
          
            context.moveTo(start+width+interval,mid_horizon);
            context.lineTo(start+width+interval-x,mid_horizon-y); 
          }
          
       
        
          var startAngle = 1.2 * Math.PI;
          var endAngle = 1 * Math.PI;
          var counterClockwise = true;
      
          context.stroke();
            
          context.beginPath();
          context.arc(start+width/4+radius*0.81, mid_horizon+width/2, radius, startAngle, endAngle, counterClockwise);

          context.moveTo(start+width/4+radius*0.81-radius,mid_horizon+width/2);
          context.lineTo(start+width/4+radius*0.81-radius-x,mid_horizon-y+width/2);

          context.moveTo(start+width/4+radius*0.81-radius,mid_horizon+width/2);
          context.lineTo(start+width/4+radius*0.81-radius+x,mid_horizon-y+width/2); 

          context.font = ' 15pt Calibri';
          context.fillStyle = '#859900';
          context.fillText(name.charAt(name_counter), start+width/4+radius*0.81-radius-width/14, mid_horizon+width/2+width/2.1);

          context.lineWidth=1;
          context.strokeStyle='#F9BF3B';
          context.stroke();
          start+=width/2*2+interval;      
          name_counter+=1;
        }
      }

    </script>

  <!-- JavaScript at the bottom for fast page loading -->

  <!-- Grab Google CDN's jQuery, with a protocol relative URL; fall back to local if offline -->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.6.2/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="js/libs/jquery-1.6.2.min.js"><\/script>')</script>


  <!-- scripts concatenated and minified via ant build script-->
  <script defer src="/javascripts/app.js"></script>
  <!-- end scripts-->

	

  <!-- Prompt IE 6 users to install Chrome Frame. Remove this if you want to support IE 6.
       chromium.org/developers/how-tos/chrome-frame-getting-started -->
  <!--[if lt IE 7 ]>
    <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
    <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
  <![endif]-->
 

  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39225214-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>



</body>
</html>

