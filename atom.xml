<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Weblog of Volkan Cirik]]></title>
  <link href="http://wolet.github.com/atom.xml" rel="self"/>
  <link href="http://wolet.github.com/"/>
  <updated>2016-01-16T21:20:16-05:00</updated>
  <id>http://wolet.github.com/</id>
  <author>
    <name><![CDATA[Volkan Cirik]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Representation Learning: A Review and New Perspectives]]></title>
    <link href="http://wolet.github.com/blog/representation-learning-a-review-and-new-perspectives/"/>
    <updated>2016-01-16T18:07:09-05:00</updated>
    <id>http://wolet.github.com/blog/representation-learning-a-review-and-new-perspectives</id>
    <content type="html"><![CDATA[<p>The first reading of the semester is from Bengio et. al. <a href="http://arxiv.org/pdf/1206.5538.pdf">“Representation Learning: A Review and New
Perspectives”</a>. The paper’s motivation is threefold : what are the 1) <em>right objectives</em> to learn <em>good representations</em>, 2) how do we compute these representations, 3) what is the connection between <strong>representation learning</strong>, <strong>density estimation</strong> and <strong>manifold learning</strong>.</p>

<hr />

<p>How we represent the data to a Machine Learning (ML) model’s determines its sense of domain, consequently, its success. For many domains, feature engineering is one of the earliers steps in a ML pipeline. This labor-intensive effort shows that the existing ML algorithms, or the setup, is not able to extract discrimative information from raw data. To make prograss towards Artificial Intelligence (AI), our solutions shout be able to <strong>identify and disentagle the underlying explanatory hidden factors</strong> of the raw data.</p>

<p>We observe the impact of Representation Learning (RL) and Deep Learning (DL) in different domains. Early success of these models are in Speech Recognition. Now, we have many solutions for masses (such as Siri, Cortana, Google Now and Echo). The resurgence of DL models happenned on Object Recognition <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">(Krizhevsky et al., 2012)</a>. One of the main focus of the DL community is Natural Language Processing (NLP) and <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">Bengio et. al. 2003</a> and <a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">Collobert et al. 2011</a> are prominent examples of the success of DL. Most notably, the success of DL models on Multi-Task and Transfer Learning challenges shows that the RL algorithms resulting representations capture underlying factors which may be relevant to different tasks. I think any DL-based models using pre-trained word vectors (whether the model finetunes it or not) are supporting this claim of the paper.</p>

<p>A recipe for good representation is below. I’ll shortly review each of the bullet points:</p>

<ul>
  <li>Must satisfy some of the priors about the world around us</li>
  <li>Smoothness and the Curse Of Dimensionality</li>
  <li>Distributed Representations</li>
  <li>Depth and Abstraction</li>
  <li>Disentangling Factors of Variation</li>
  <li>Good Criteria For Learning Representations</li>
</ul>

<p><strong>Priors for Representation Learning in AI</strong>
Let’s introduce some notation before explaining priors. Our aim to learn a function $f$ of input $X$ to output $Y$. $x$ is an instance of input $X$ and the value we want to learn with $f$ is $y$. A good representation should satisft some of the below priors:</p>

<ul>
  <li><strong>smoothness</strong> : for $x \approx y$ implies $f(x) \approx f(y)$.</li>
  <li><strong>multiple explanatory factors</strong> : the distribution generating the data has a set of underlying factors.</li>
  <li><strong>a hierarchical organization of explanatory factors</strong> : the abstract concepts explaining the world can be decomposed into less abstract ones.</li>
  <li><strong>semi-supervised learning</strong> : things explaining $P(X)$ tend to be useful for learning $P(Y \mid X)$. (e.g word embeddings learned during Language Modelling (LM) in general useful for supervised tasks).</li>
  <li><strong>shared factors across tasks</strong> : the factors explaining $P(Y \mid X,task)$, shared with other tasks (e.g multi-task learning)</li>
  <li><strong>manifolds</strong> : there is a smaller space than original space where the probability mass is concentrated</li>
  <li><strong>natural clustering</strong> : local variations on the manifold tend to preserve the category of the instances that live on that manifold</li>
  <li><strong>temporal &amp; spacial coherence</strong> : consecutive or spatially nearby observations tend to be in the same neighborhood on the manifold, thus preserving relevant categorical concepts</li>
  <li><strong>sparsity</strong> : only a small subset of factors are <em>active</em> or relevant for an instance $x$.</li>
  <li><strong>simplicity of factor dependencies</strong> : factors explaining the data, are related to each other through simple (e.g linear) dependencies.</li>
</ul>

<p><strong>Smoothness and the Curse Of Dimensionality</strong> : Non-parametric learners assume that $f$ is smooth enough and they can rely on examples to expilicityl map out the wrinkles (i.e how it behaves around instances) of the target function. But, the number of such wrinkles (i.e variance of $f$) may grow exponentially with the number of interacting factors when the represented data in raw space. Instead, they propose to use these non-parametric learners on top of learned representations. Learned representations will provide the similarity metric which is the core need of a non-parametric model such as kernel machines.</p>

<p><strong>Distributed Representations</strong> : Distributed representations provide an exponential gain in learning the hyperplanes separating the data into regions.</p>

<p><img src="https://dl.dropboxusercontent.com/s/p40jirtw49ecz7g/distributed_representations.png" alt="distributed representations rox" /></p>

<p>Let me elaborate using above example from <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf">Bengio 2009</a>. Using only 3 features, one can separate 7 regions. Unlike learners of one hot representations which all require $O(N)$ parameters to distinguish $O(N)$ input regions, learners of distributed representations e.g RBMs, can distinguish $O(2^k)$ input regions using only $O(N)$ parameters. The main reason is each unit/region separator benefits from many examples, even if these examples are not neighbors at all, for tuning the parameters. Another way to look at it : for a sample $x$, a subset of features/units/region separators will be activated thus will be used to learn their parameters.</p>

<p><strong>Depth and Abstraction</strong> : The depth, the length of the longest path from input node to output node, provides two advantages. First, feature re-use, which I don’t fully understand. However, one remark is very important. Deep models learn exponantially more efficient than shallow counterparts which suggest that they require fewer parameters, consequently, can be learned with fewer examples. Second, in deep architectures, each level provides the ground for more abstract representations. More abstract concepts are usually sensitive to specific changes on the lower concepts (invariance).</p>

<p><strong>Disentangling Factors of Variation</strong> : A good representation should disentangle as many factors as possible while discarding as little information about the data as useful. This smells a lot like allegedly <a href="http://quoteinvestigator.com/2011/05/13/einstein-simple/">Einstein’s simplicity quote</a> and the concept of Occam’s Razor.</p>

<p><strong>Good Criteria For Learning Representations</strong> : We hope a good representation satisfy many criteria but it is an open problem to turn these criteria into objectives in an ML model.</p>

<p>(to be continued)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hello World!]]></title>
    <link href="http://wolet.github.com/blog/hello-world/"/>
    <updated>2016-01-15T17:36:59-05:00</updated>
    <id>http://wolet.github.com/blog/hello-world</id>
    <content type="html"><![CDATA[<p>hi! the shorter version of hello.</p>

<p>in my last semester at cmu i am taking advanced multi-modal machine learning. it is a mixture of seminar+lab course i.e you read a bunch of papers + get hands on experience with problems/models. one of the requirements is that you have to post a short summary of each week’s reading to instructors. i plan to use this as an excuse to keep a blag. we’ll see how it goes.</p>

<p>and here’s a test for a math equation 
<script type="math/tex">x_t = x_{t-1} + y_t</script></p>
]]></content>
  </entry>
  
</feed>
